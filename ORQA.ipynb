{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating LLM Reasoning in the Operations Research Domain with ORQA\n",
    "\n",
    "Operations Research Question Answering (ORQA) is a new benchmark designed to assess the reasoning capabilities of Large Language Models (LLMs) in a specialized technical domain, namely Operations Research (OR). The benchmark evaluates whether LLMs can emulate the knowledge and reasoning skills of OR experts when presented with complex optimization problems. Crafted by OR experts, the dataset consists of real-world optimization problems that require multi-step mathematical reasoning to arrive at solutions. Our evaluations of several open-source LLMs, such as LLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting a gap in their ability to generalize to specialized technical domains.\n",
    "\n",
    "\n",
    "ORQA questions are hand-crafted to require complex, multi-step reasoning to identify the components of mathematical models and their interrelationships. An example of these components and their corresponding mathematical formulations is shown below.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://vbdai-notebooks.obs.cn-north-4.myhuaweicloud.com/orqa/img/ORQA-Fig2.png\" width=\"1000\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Download Code and Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://vbdai-notebooks.obs.cn-north-4.myhuaweicloud.com/orqa/code.zip\n",
    "!unzip -qo code.zip\n",
    "\n",
    "!wget https://vbdai-notebooks.obs.cn-north-4.myhuaweicloud.com/orqa/dataset.zip\n",
    "!unzip -qo dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Environment Setup\n",
    "\n",
    "### Step 1: Create and Activate the Conda Environment\n",
    "\n",
    "First, create and activate a conda environment with Python 3.11.4:\n",
    "\n",
    "```bash\n",
    "conda create --name orqa_py3.11 python=3.11.4\n",
    "conda activate orqa_py3.11\n",
    "```\n",
    "\n",
    "### Step 2: Install Dependencies\n",
    "\n",
    "If your device uses **CUDA Version 12.2**, you can install all dependencies from the `requirements.txt` file:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you have a different CUDA version or if the above command doesn't work, you can install the necessary packages individually:\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub tenacity evaluate\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset can be found in the directory: `src/task/dataset`.\n",
    "\n",
    "It includes two files:\n",
    "\n",
    "- **Test Set (1468 instances)**: `ORQA_test.jsonl`\n",
    "- **Validation Set (45 instances)**: `ORQA_validation.jsonl`\n",
    "\n",
    "### Each Dataset Instance Contains:\n",
    "\n",
    "1. **CONTEXT**: A description of an optimization problem presented as a case study in natural language.\n",
    "\n",
    "2. **QUESTION**: A question related to the problem's specifications, underlying model components, or the logic of the optimization model. It might ask about:\n",
    "   - Objective criteria or constraints\n",
    "   - Model components (e.g., elements in the optimization)\n",
    "   - Relationships between components\n",
    "\n",
    "3. **OPTIONS**: A list of four possible answers. These are created by OR experts to make the question challenging. The LLM must choose the correct answer from these options.\n",
    "\n",
    "4. **TARGET_ANSWER**: The correct answer to the question.\n",
    "\n",
    "5. **REASONING**: For validation set only, which contains expert-created reasoning steps that explain how the correct answer is derived.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://vbdai-notebooks.obs.cn-north-4.myhuaweicloud.com/orqa/img/data_breakdown.PNG\" width=\"1000\" />\n",
    "</p>\n",
    "\n",
    "### Example Instance (Validation Set)\n",
    "\n",
    "Below is an example instance from the validation split, which includes expert-created reasoning steps used for in-context learning. **Note**: The test set instances do not contain these reasoning steps.\n",
    "\n",
    "```json\n",
    "instance = {\n",
    "  \"QUESTION_TYPE\": \"Q6\", \n",
    "  \"CONTEXT\": \"As a programming director at the Starlight Network, you're tasked with creating a lineup for the prime-time broadcasting...\",\n",
    "  \"QUESTION\": \" What are the decision activities of the optimization problem?\",\n",
    "  \"OPTIONS\": [\"Due date\", \"Show broadcast order\", \"Show broadcast indicator\", \"Processing time\"], \n",
    "  \"ARGET_ANSWER\": 2, \n",
    "  \"REASONING\": \"The possible decision activities mentioned in options ...\"\n",
    "}\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Prompt (Implemented in the Code)\n",
    "\n",
    "The **prompt** is constructed using specific keys from the dataset. Below is how prompts are buildin our experiemnts.\n",
    "\n",
    "### **Standard (0-shot) Prompting**\n",
    "\n",
    "The prompt is built in `/src/task/base_task.py` and `/src/task/standard_task.py`. The format for the prompt is as follows:\n",
    "\n",
    "```python\n",
    "Given the context (following Context:), select the most appropriate answer to the question (following Question:). Answer only 'A', 'B', 'C', or 'D'\n",
    "Context: {instance[\"CONTEXT\"]}\n",
    "Question: {instance[\"question\"]}\n",
    "A. {instance[\"OPTIONS\"][0]}\n",
    "B. {instance[\"OPTIONS\"][1]}\n",
    "C. {instance[\"OPTIONS\"][2]}\n",
    "D. {instance[\"OPTIONS\"][3]}\n",
    "Answer: Among A through D, the answer is (\n",
    "```\n",
    "\n",
    "### **CoT (0-shot) Prompting**\n",
    "\n",
    "The prompts are built in `/src/task/base_task.py` and `/src/task/cot_task.py`.\n",
    "\n",
    "<u>First (Reasoning Eliciting) Prompt:</u>\n",
    "\n",
    "```python\n",
    "Given the context (following Context:), provide the chain of thoughts (following Reasoning:) to solve the question (following Question:). Remember, only one option is correct.\n",
    "Context: {instance[\"CONTEXT\"]}\n",
    "Question: {instance[\"question\"]}\n",
    "A. {instance[\"OPTIONS\"][0]}\n",
    "B. {instance[\"OPTIONS\"][1]}\n",
    "C. {instance[\"OPTIONS\"][2]}\n",
    "D. {instance[\"OPTIONS\"][3]}\n",
    "Reasoning: {os.environ['trigger']}\n",
    "```\n",
    "\n",
    "- The **trigger prompt** is by default set to `\"Let's think step by step\"`.\n",
    "\n",
    "<u>Second (answering) prompt:</u>\n",
    "\n",
    "Let the output reasoning of the reasoning eliciting prompt be `REASONING`.\n",
    "\n",
    "```python\n",
    "Given the context (following Context:), the reasoning (following Reasoning:), select the most appropriate answer to the question (following Question:). Answer only 'A', 'B', 'C', or 'D'. There is only one correct answer.\n",
    "Context: {instance[\"CONTEXT\"]}\n",
    "Question: {instance[\"QUESTION\"]}\n",
    "A. {instance[\"OPTIONS\"][0]}\n",
    "B. {instance[\"OPTIONS\"][1]}\n",
    "C. {instance[\"OPTIONS\"][2]}\n",
    "D. {instance[\"OPTIONS\"][3]}\n",
    "Reasoning: {os.environ['trigger']}. {REASONING}\n",
    "Answer: Among A through D, the answer is (\n",
    "``` \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments\n",
    "\n",
    "Before running the experiments, ensure you pass your Huggingface API token either as an argument (`--token hf_xxxxxx`) or set it as a default in `standard_inference.py` and/or `cot_inference.py`.\n",
    "\n",
    "- `cot_inference.py`: Runs COT experiments\n",
    "- `standard_inference.py`: Runs standard n-shot experiments\n",
    "- Both scripts use `final_inference_utils.py` for global variables and utility functions, including model names.\n",
    "\n",
    "### Running the Experiments\n",
    "\n",
    "To generate the main accuracy results for each model, run the following scripts:\n",
    "\n",
    "```bash\n",
    "bash src/standard_0-shot.sh\n",
    "bash src/standard_1-shot.sh\n",
    "bash src/standard_3-shot.sh\n",
    "bash src/cot_0-shot.sh\n",
    "bash src/cot_1-shot.sh\n",
    "```\n",
    "\n",
    "The table below shows the accuracy of different models across various prompting strategies.\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://vbdai-notebooks.obs.cn-north-4.myhuaweicloud.com/orqa/img/table_2.PNG\" width=\"500\" />\n",
    "</p>\n",
    "\n",
    "### Trigger Prompt Analysis\n",
    "\n",
    "To analyze the impact of different trigger prompts on accuracy, run the following script:\n",
    "\n",
    "```bash\n",
    "bash src/trigger_prompt_analysis.sh\n",
    "```\n",
    "\n",
    "The results for the Trigger Prompt are averaged over five independent runs per prompt. The final row displays the accuracy of ensembling all prompts.\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://vbdai-notebooks.obs.cn-north-4.myhuaweicloud.com/orqa/img/table_3.PNG\" width=\"700\" />\n",
    "</p>\n",
    "\n",
    "### Results Storage\n",
    "\n",
    "All results will be saved in `/src/output_of_llms`."
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "6b98c56e-913b-47ef-8d9f-3266c8aec06a"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "GPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54099",
   "name": "pytorch1.4-cuda10.1-cudnn7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "python-3.7.10",
   "language": "python",
   "name": "python-3.7.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
